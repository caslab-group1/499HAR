{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rFv292X85ewG","executionInfo":{"status":"ok","timestamp":1647895729031,"user_tz":240,"elapsed":3332,"user":{"displayName":"tingyu chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13919754884247043496"}},"outputId":"c11a0289-1674-4bd1-a6c2-f4957f511640"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FsePPpwZSmqt"},"outputs":[],"source":["!pip install pyyaml==5.1\n","\n","import torch\n","TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n","CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n","print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n","# Install detectron2 that matches the above pytorch version\n","# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n","!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/$CUDA_VERSION/torch$TORCH_VERSION/index.html\n","# If there is not yet a detectron2 release that matches the given torch + CUDA version, you need to install a different pytorch.\n","\n","exit(0)  # After installation, you may need to \"restart runtime\" in Colab. This line can also restart runtime\n","#After restarting runtime, you don't need to run this cell again."]},{"cell_type":"markdown","source":["Install Detectron2 for generating 2D poses."],"metadata":{"id":"cOBkwdbhPlAk"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"ZyAvNCJMmvFF","executionInfo":{"status":"ok","timestamp":1647895792959,"user_tz":240,"elapsed":2702,"user":{"displayName":"tingyu chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13919754884247043496"}}},"outputs":[],"source":["# Some basic setup:\n","# Setup detectron2 logger\n","import detectron2\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","\n","# import some common libraries\n","import numpy as np\n","import os, json, cv2, random\n","from google.colab.patches import cv2_imshow\n","\n","# import some common detectron2 utilities\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import MetadataCatalog, DatasetCatalog"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":691,"status":"ok","timestamp":1647895876310,"user":{"displayName":"tingyu chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13919754884247043496"},"user_tz":240},"id":"RN3-NoI92kM0","outputId":"73f5dad2-ebd1-450a-9535-3d42ea724ea7"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/videopose/VideoPose3D/inference\n"]}],"source":["%cd /content/drive/My Drive/videopose/VideoPose3D/inference"]},{"cell_type":"markdown","source":["preprocessing video for generating 2D data\n","  * get videos from inference/inputvideos\n","  * export the result into inference/prevideos"],"metadata":{"id":"HbQwdwa06KVh"}},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41658,"status":"ok","timestamp":1647896140586,"user":{"displayName":"tingyu chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13919754884247043496"},"user_tz":240},"id":"UHTOmuIYhkI9","outputId":"e9b1d541-2293-4825-9e47-3b3b7edcc14d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing inputvideos/S001C002P003R002A023.mp4\n","ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n","  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n","  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n","  libavutil      55. 78.100 / 55. 78.100\n","  libavcodec     57.107.100 / 57.107.100\n","  libavformat    57. 83.100 / 57. 83.100\n","  libavdevice    57. 10.100 / 57. 10.100\n","  libavfilter     6.107.100 /  6.107.100\n","  libavresample   3.  7.  0 /  3.  7.  0\n","  libswscale      4.  8.100 /  4.  8.100\n","  libswresample   2.  9.100 /  2.  9.100\n","  libpostproc    54.  7.100 / 54.  7.100\n","Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'inputvideos/S001C002P003R002A023.mp4':\n","  Metadata:\n","    major_brand     : qt  \n","    minor_version   : 0\n","    compatible_brands: qt  \n","    creation_time   : 2022-02-28T02:46:38.000000Z\n","  Duration: 00:00:02.16, start: 0.000000, bitrate: 7208 kb/s\n","    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709), 1080x720 [SAR 1:1 DAR 3:2], 7018 kb/s, 25.91 fps, 30 tbr, 600 tbn, 1200 tbc (default)\n","    Metadata:\n","      creation_time   : 2022-02-28T02:46:38.000000Z\n","      handler_name    : Core Media Data Handler\n","      encoder         : H.264\n","    Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, mono, fltp, 60 kb/s (default)\n","    Metadata:\n","      creation_time   : 2022-02-28T02:46:38.000000Z\n","      handler_name    : Core Media Data Handler\n","Stream mapping:\n","  Stream #0:0 -> #0:0 (h264 (native) -> rawvideo (native))\n","Press [q] to stop, [?] for help\n","Output #0, image2pipe, to 'pipe:':\n","  Metadata:\n","    major_brand     : qt  \n","    minor_version   : 0\n","    compatible_brands: qt  \n","    encoder         : Lavf57.83.100\n","    Stream #0:0(und): Video: rawvideo (BGR[24] / 0x18524742), bgr24, 1080x720 [SAR 1:1 DAR 3:2], q=2-31, 559872 kb/s, 30 fps, 30 tbn, 30 tbc (default)\n","    Metadata:\n","      creation_time   : 2022-02-28T02:46:38.000000Z\n","      handler_name    : Core Media Data Handler\n","      encoder         : Lavc57.107.100 rawvideo\n","/usr/local/lib/python3.7/dist-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  max_size = (max_size + (stride - 1)) // stride * stride\n","/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","/usr/local/lib/python3.7/dist-packages/detectron2/structures/keypoints.py:224: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  y_int = (pos - x_int) // w\n","Frame 0 processed in 0.625s\n","Frame 1 processed in 0.528s\n","Frame 2 processed in 0.522s\n","Frame 3 processed in 0.511s\n","Frame 4 processed in 0.499s\n","Frame 5 processed in 0.497s\n","Frame 6 processed in 0.486s\n","Frame 7 processed in 0.492s\n","Frame 8 processed in 0.489s\n","Frame 9 processed in 0.493s\n","Frame 10 processed in 0.491s\n","Frame 11 processed in 0.491s\n","Frame 12 processed in 0.493s\n","Frame 13 processed in 0.494s\n","Frame 14 processed in 0.495s\n","Frame 15 processed in 0.501s\n","Frame 16 processed in 0.496s\n","Frame 17 processed in 0.501s\n","Frame 18 processed in 0.490s\n","Frame 19 processed in 0.505s\n","Frame 20 processed in 0.496s\n","Frame 21 processed in 0.492s\n","Frame 22 processed in 0.493s\n","Frame 23 processed in 0.500s\n","Frame 24 processed in 0.492s\n","Frame 25 processed in 0.499s\n","Frame 26 processed in 0.495s\n","Frame 27 processed in 0.499s\n","Frame 28 processed in 0.496s\n","Frame 29 processed in 0.501s\n","Frame 30 processed in 0.491s\n","Frame 31 processed in 0.495s\n","Frame 32 processed in 0.497s\n","Frame 33 processed in 0.511s\n","Frame 34 processed in 0.493s\n","Frame 35 processed in 0.508s\n","Frame 36 processed in 0.494s\n","Frame 37 processed in 0.506s\n","Frame 38 processed in 0.492s\n","Frame 39 processed in 0.507s\n","Frame 40 processed in 0.495s\n","Frame 41 processed in 0.502s\n","Frame 42 processed in 0.500s\n","Frame 43 processed in 0.504s\n","Frame 44 processed in 0.495s\n","Frame 45 processed in 0.501s\n","Frame 46 processed in 0.500s\n","Frame 47 processed in 0.502s\n","Frame 48 processed in 0.503s\n","Frame 49 processed in 0.500s\n","Frame 50 processed in 0.496s\n","Frame 51 processed in 0.507s\n","Frame 52 processed in 0.498s\n","Frame 53 processed in 0.496s\n","Frame 54 processed in 0.495s\n","frame=   56 fps=2.0 q=-0.0 Lsize=  127575kB time=00:00:02.16 bitrate=482351.2kbits/s speed=0.0782x    \n","video:127575kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000000%\n","Frame 55 processed in 0.503s\n","/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py:719: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  val = np.asanyarray(val)\n","Processing inputvideos/S001C002P003R002A001.mp4\n","ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n","  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n","  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n","  libavutil      55. 78.100 / 55. 78.100\n","  libavcodec     57.107.100 / 57.107.100\n","  libavformat    57. 83.100 / 57. 83.100\n","  libavdevice    57. 10.100 / 57. 10.100\n","  libavfilter     6.107.100 /  6.107.100\n","  libavresample   3.  7.  0 /  3.  7.  0\n","  libswscale      4.  8.100 /  4.  8.100\n","  libswresample   2.  9.100 /  2.  9.100\n","  libpostproc    54.  7.100 / 54.  7.100\n","Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'inputvideos/S001C002P003R002A001.mp4':\n","  Metadata:\n","    major_brand     : isom\n","    minor_version   : 512\n","    compatible_brands: isomiso2avc1mp41\n","    encoder         : Lavf57.83.100\n","  Duration: 00:00:09.99, start: 0.000000, bitrate: 111 kb/s\n","    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 490x360 [SAR 1:1 DAR 49:36], 313 kb/s, 29.97 fps, 29.97 tbr, 30k tbn, 59.94 tbc (default)\n","    Metadata:\n","      handler_name    : VideoHandler\n","    Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 96 kb/s (default)\n","    Metadata:\n","      handler_name    : SoundHandler\n","Stream mapping:\n","  Stream #0:0 -> #0:0 (h264 (native) -> rawvideo (native))\n","Press [q] to stop, [?] for help\n","Output #0, image2pipe, to 'pipe:':\n","  Metadata:\n","    major_brand     : isom\n","    minor_version   : 512\n","    compatible_brands: isomiso2avc1mp41\n","    encoder         : Lavf57.83.100\n","    Stream #0:0(und): Video: rawvideo (BGR[24] / 0x18524742), bgr24, 490x360 [SAR 1:1 DAR 49:36], q=2-31, 126881 kb/s, 29.97 fps, 29.97 tbn, 29.97 tbc (default)\n","    Metadata:\n","      handler_name    : VideoHandler\n","      encoder         : Lavc57.107.100 rawvideo\n","/usr/local/lib/python3.7/dist-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  max_size = (max_size + (stride - 1)) // stride * stride\n","/usr/local/lib/python3.7/dist-packages/detectron2/structures/keypoints.py:224: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  y_int = (pos - x_int) // w\n","Frame 0 processed in 0.475s\n","Frame 1 processed in 0.467s\n","Frame 2 processed in 0.472s\n","Frame 3 processed in 0.475s\n","Frame 4 processed in 0.471s\n","Frame 5 processed in 0.476s\n","Frame 6 processed in 0.477s\n","Frame 7 processed in 0.482s\n","Frame 8 processed in 0.469s\n","Frame 9 processed in 0.471s\n","Frame 10 processed in 0.481s\n","frame=   12 fps=2.3 q=-0.0 Lsize=    6202kB time=00:00:00.40 bitrate=126881.1kbits/s speed=0.0765x    \n","video:6202kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000000%\n","Frame 11 processed in 0.484s\n","/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py:719: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  val = np.asanyarray(val)\n"]}],"source":["!python infer_video_d2.py \\\n","    --cfg COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x.yaml \\\n","    --output-dir pre_videos \\\n","    --image-ext mp4 \\\n","    inputvideos"]},{"cell_type":"markdown","source":["copy files for generating 2D data"],"metadata":{"id":"Ca086jVZ6UwK"}},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":674,"status":"ok","timestamp":1647896158676,"user":{"displayName":"tingyu chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13919754884247043496"},"user_tz":240},"id":"dC_SNdiC7Yey","outputId":"c6a167a7-1adc-420c-a4e1-d96f38bbc1a9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/drive/My Drive/videopose/VideoPose3D/data/input/S001C002P003R002A023.mp4.npz',\n"," '/content/drive/My Drive/videopose/VideoPose3D/data/input/S001C002P003R002A001.mp4.npz']"]},"metadata":{},"execution_count":6}],"source":["from distutils.dir_util import copy_tree\n","copy_tree(\"/content/drive/My Drive/videopose/VideoPose3D/inference/pre_videos\",\"/content/drive/My Drive/videopose/VideoPose3D/data/input\")"]},{"cell_type":"code","execution_count":101,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":892,"status":"ok","timestamp":1647906227186,"user":{"displayName":"tingyu chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13919754884247043496"},"user_tz":240},"id":"Bf2GARe3q2iy","outputId":"79df71cd-3992-456d-e662-70c28cbf9b0f"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/videopose/VideoPose3D/data\n"]}],"source":["%cd '/content/drive/My Drive/videopose/VideoPose3D/data'"]},{"cell_type":"markdown","source":["generating 2D poses\n","  * -i: input file\n","  * -o: output file"],"metadata":{"id":"W5kxmXPC6dZp"}},{"cell_type":"code","execution_count":102,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":683,"status":"ok","timestamp":1647906235842,"user":{"displayName":"tingyu chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13919754884247043496"},"user_tz":240},"id":"V4o85HNgqq13","outputId":"eb929b53-8c1e-4f4b-de2b-442d2c5d3e5d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Parsing 2D detections from input\n","Processing input/S001C002P003R002A023.mp4.npz\n","56 total frames processed\n","0 frames were interpolated\n","----------\n","Processing input/S001C002P003R002A001.mp4.npz\n","12 total frames processed\n","0 frames were interpolated\n","----------\n","Saving...\n","Done.\n"]}],"source":["!python prepare_data_2d_custom.py -i input -o twoDposes"]},{"cell_type":"code","execution_count":103,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":840,"status":"ok","timestamp":1647906241226,"user":{"displayName":"tingyu chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13919754884247043496"},"user_tz":240},"id":"ZkmZRu2n70zl","outputId":"2a2b488d-c954-4ce2-e362-2168c08bc2a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/videopose/VideoPose3D\n"]}],"source":["%cd '/content/drive/My Drive/videopose/VideoPose3D'"]},{"cell_type":"code","execution_count":104,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5283,"status":"ok","timestamp":1647906249102,"user":{"displayName":"tingyu chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13919754884247043496"},"user_tz":240},"id":"K3Fjrs-Nvo7r","outputId":"90a4e7ea-499d-4957-bd0d-43a35312352e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(actions='*', architecture='3,3,3,3,3', batch_size=1024, bone_length_term=True, by_subject=False, causal=False, channels=1024, checkpoint='checkpoint', checkpoint_frequency=10, data_augmentation=True, dataset='custom', dense=False, disable_optimizations=False, downsample=1, dropout=0.25, epochs=60, evaluate='pretrained_h36m_detectron_coco.bin', export_training_curves=False, keypoints='twoDposes', learning_rate=0.001, linear_projection=False, lr_decay=0.95, no_eval=False, no_proj=False, render=True, resume='', stride=1, subjects_test='S9,S11', subjects_train='S1,S5,S6,S7,S8', subjects_unlabeled='', subset=1, test_time_augmentation=True, viz_action='custom', viz_bitrate=3000, viz_camera=0, viz_downsample=1, viz_export=None, viz_limit=-1, viz_no_ground_truth=False, viz_output=None, viz_size=5, viz_skip=0, viz_subject=None, viz_video=None, warmup=1)\n","Loading dataset...\n","Preparing data...\n","Loading 2D detections...\n","S001C002P003R002A023.mp4\n","INFO: Receptive field: 243 frames\n","INFO: Trainable parameter count: 16952371\n","Loading checkpoint checkpoint/pretrained_h36m_detectron_coco.bin\n","This model was trained for 80 epochs\n","INFO: Testing on 56 frames\n","S001C002P003R002A001.mp4\n","INFO: Receptive field: 243 frames\n","INFO: Trainable parameter count: 16952371\n","Loading checkpoint checkpoint/pretrained_h36m_detectron_coco.bin\n","This model was trained for 80 epochs\n","INFO: Testing on 12 frames\n","Rendering...\n","INFO: this action is unlabeled. Ground truth will not be rendered.\n","Read file End or Error\n","INFO: this action is unlabeled. Ground truth will not be rendered.\n","Read file End or Error\n"]}],"source":["!python run.py -d custom \\\n","                -k twoDposes \\\n","                -arc 3,3,3,3,3 \\\n","                -c checkpoint \\\n","                --evaluate pretrained_h36m_detectron_coco.bin \\\n","                --render \\\n","                --viz-action custom \\\n","                --viz-camera 0 \n","                #--viz-video inference/inputvideos/ mp4 \\\n","                #--viz-export '/content/drive/My Drive/videopose/VideoPose3D/final_output'\n","                #--viz-output drinking.mp4 \\\n","                #--viz-size 6\n","                #--viz-subject inference/inputvideos/ mp4 \\"]},{"cell_type":"code","source":["from distutils.dir_util import copy_tree\n","copy_tree(\"/content/drive/My Drive/videopose/VideoPose3D/final_output\",\"/content/drive/My Drive/MS-G3D/data/nturgbd_raw/nturgb+d_skeletons120\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cFabm3K7mtPH","executionInfo":{"status":"ok","timestamp":1647906458510,"user_tz":240,"elapsed":804,"user":{"displayName":"tingyu chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13919754884247043496"}},"outputId":"91f4eb00-8d31-4639-8953-4882aaff31dd"},"execution_count":108,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/drive/My Drive/MS-G3D/data/nturgbd_raw/nturgb+d_skeletons120/S001C002P003R002A023.skeleton',\n"," '/content/drive/My Drive/MS-G3D/data/nturgbd_raw/nturgb+d_skeletons120/S001C002P003R002A001.skeleton']"]},"metadata":{},"execution_count":108}]},{"cell_type":"markdown","source":["MS-G3D"],"metadata":{"id":"nieI-r-WA4GE"}},{"cell_type":"code","source":["%cd '/content/drive/My Drive/MS-G3D/data_gen'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SxvFFspP3r2z","executionInfo":{"status":"ok","timestamp":1647906887850,"user_tz":240,"elapsed":786,"user":{"displayName":"tingyu chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13919754884247043496"}},"outputId":"6b4f2697-5fdc-483e-d0c9-e7d27fae8bed"},"execution_count":121,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/MS-G3D/data_gen\n"]}]},{"cell_type":"code","source":["!python3 ntu120_gendata.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LcctLNgS3wP7","executionInfo":{"status":"ok","timestamp":1647906897816,"user_tz":240,"elapsed":762,"user":{"displayName":"tingyu chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13919754884247043496"}},"outputId":"ed809781-3e7a-4985-d600-6da65bc480dc"},"execution_count":122,"outputs":[{"output_type":"stream","name":"stdout","text":["xsub train\n","\r0it [00:00, ?it/s]\r0it [00:00, ?it/s]\n","pad the null frames with the previous frames\n","\r0it [00:00, ?it/s]\r0it [00:00, ?it/s]\n","sub the center joint #1 (spine joint in ntu and neck joint in kinetics)\n","\r0it [00:00, ?it/s]\r0it [00:00, ?it/s]\n","parallel the bone between hip(jpt 0) and spine(jpt 1) of the first person to the z axis\n","\r0it [00:00, ?it/s]\r0it [00:00, ?it/s]\n","parallel the bone between right shoulder(jpt 8) and left shoulder(jpt 4) of the first person to the x axis\n","\r0it [00:00, ?it/s]\r0it [00:00, ?it/s]\n","xsub val\n","\r  0% 0/2 [00:00<?, ?it/s]\r100% 2/2 [00:00<00:00, 204.88it/s]\n","pad the null frames with the previous frames\n","\r  0% 0/2 [00:00<?, ?it/s]\r100% 2/2 [00:00<00:00, 2089.32it/s]\n","sub the center joint #1 (spine joint in ntu and neck joint in kinetics)\n","\r  0% 0/2 [00:00<?, ?it/s]\r100% 2/2 [00:00<00:00, 3377.06it/s]\n","parallel the bone between hip(jpt 0) and spine(jpt 1) of the first person to the z axis\n","\r  0% 0/2 [00:00<?, ?it/s]\r100% 2/2 [00:00<00:00, 45.50it/s]\n","parallel the bone between right shoulder(jpt 8) and left shoulder(jpt 4) of the first person to the x axis\n","\r  0% 0/2 [00:00<?, ?it/s]\r100% 2/2 [00:00<00:00, 45.72it/s]\n","xset train\n","\r0it [00:00, ?it/s]\r0it [00:00, ?it/s]\n","pad the null frames with the previous frames\n","\r0it [00:00, ?it/s]\r0it [00:00, ?it/s]\n","sub the center joint #1 (spine joint in ntu and neck joint in kinetics)\n","\r0it [00:00, ?it/s]\r0it [00:00, ?it/s]\n","parallel the bone between hip(jpt 0) and spine(jpt 1) of the first person to the z axis\n","\r0it [00:00, ?it/s]\r0it [00:00, ?it/s]\n","parallel the bone between right shoulder(jpt 8) and left shoulder(jpt 4) of the first person to the x axis\n","0it [00:00, ?it/s]\n","xset val\n","100% 2/2 [00:00<00:00, 162.65it/s]\n","pad the null frames with the previous frames\n","100% 2/2 [00:00<00:00, 2386.52it/s]\n","sub the center joint #1 (spine joint in ntu and neck joint in kinetics)\n","100% 2/2 [00:00<00:00, 3634.58it/s]\n","parallel the bone between hip(jpt 0) and spine(jpt 1) of the first person to the z axis\n","100% 2/2 [00:00<00:00, 47.57it/s]\n","parallel the bone between right shoulder(jpt 8) and left shoulder(jpt 4) of the first person to the x axis\n","100% 2/2 [00:00<00:00, 42.68it/s]\n"]}]},{"cell_type":"code","source":["!python gen_bone_data.py --dataset ntu120"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-F0_jJ9q4VpS","executionInfo":{"status":"ok","timestamp":1647906903130,"user_tz":240,"elapsed":695,"user":{"displayName":"tingyu chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13919754884247043496"}},"outputId":"72de3168-c8a3-444d-89c2-456a2d54cb39"},"execution_count":123,"outputs":[{"output_type":"stream","name":"stdout","text":["ntu120/xset train\n","\r  0% 0/25 [00:00<?, ?it/s]\r100% 25/25 [00:00<00:00, 53038.75it/s]\n","ntu120/xset val\n","\r  0% 0/25 [00:00<?, ?it/s]\r100% 25/25 [00:00<00:00, 8277.36it/s]\n","ntu120/xsub train\n","\r  0% 0/25 [00:00<?, ?it/s]\r100% 25/25 [00:00<00:00, 50291.41it/s]\n","ntu120/xsub val\n","\r  0% 0/25 [00:00<?, ?it/s]\r100% 25/25 [00:00<00:00, 8440.60it/s]\n"]}]},{"cell_type":"code","source":["%cd '/content/drive/My Drive/MS-G3D/graph'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V793EIcQ4dHj","executionInfo":{"status":"ok","timestamp":1647906905843,"user_tz":240,"elapsed":6,"user":{"displayName":"tingyu chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13919754884247043496"}},"outputId":"9bd6b1ce-0e05-4fb7-eaff-149feb03055d"},"execution_count":124,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/MS-G3D/graph\n"]}]},{"cell_type":"code","source":["!python ntu_rgb_d.py "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1XTtUXHL4eV5","executionInfo":{"status":"ok","timestamp":1647906910405,"user_tz":240,"elapsed":1557,"user":{"displayName":"tingyu chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13919754884247043496"}},"outputId":"f9df36cf-3c1e-4cdb-bdaf-d6769a37a20a"},"execution_count":125,"outputs":[{"output_type":"stream","name":"stdout","text":["<Figure size 640x480 with 3 Axes>\n","(25, 25) (25, 25) (25, 25)\n"]}]},{"cell_type":"code","source":["%cd '/content/drive/My Drive/MS-G3D/feeders'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YvrPxNaz4iHq","executionInfo":{"status":"ok","timestamp":1647906912952,"user_tz":240,"elapsed":6,"user":{"displayName":"tingyu chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13919754884247043496"}},"outputId":"ff465f4e-1773-4417-c98b-3b9599859faa"},"execution_count":126,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/MS-G3D/feeders\n"]}]},{"cell_type":"code","source":["!python feeder.py "],"metadata":{"id":"t_Ahv2TP4izv","executionInfo":{"status":"ok","timestamp":1647906917221,"user_tz":240,"elapsed":1908,"user":{"displayName":"tingyu chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13919754884247043496"}}},"execution_count":127,"outputs":[]},{"cell_type":"code","source":["%cd '/content/drive/My Drive/MS-G3D/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nj_JERLi4lC8","executionInfo":{"status":"ok","timestamp":1647906921139,"user_tz":240,"elapsed":836,"user":{"displayName":"tingyu chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13919754884247043496"}},"outputId":"8ce98e7f-03fe-4e95-b90b-0b2c5b8e4c85"},"execution_count":128,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/MS-G3D\n"]}]},{"cell_type":"code","source":["!pip install tensorboardX"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kt8uew1n4p42","executionInfo":{"status":"ok","timestamp":1647906584229,"user_tz":240,"elapsed":934,"user":{"displayName":"tingyu chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13919754884247043496"}},"outputId":"10bcfa84-92f7-47a5-e373-25583005f05b"},"execution_count":119,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorboardX\n","  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n","\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 24.4 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 71 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 92 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 102 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 112 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 122 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 4.1 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.15.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.5)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.5\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/NVIDIA/apex\n","%cd apex\n","!python setup.py install"],"metadata":{"id":"YSTPpRPc4sfM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd '/content/drive/My Drive/MS-G3D/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vGBI56sL421q","executionInfo":{"status":"ok","timestamp":1647906925538,"user_tz":240,"elapsed":681,"user":{"displayName":"tingyu chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13919754884247043496"}},"outputId":"664e6aae-f6af-445d-a1f6-6b36d0856efd"},"execution_count":129,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/MS-G3D\n"]}]},{"cell_type":"code","source":["!bash eval_pretrained.sh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4YCxgdcg44oJ","executionInfo":{"status":"ok","timestamp":1647907010045,"user_tz":240,"elapsed":81163,"user":{"displayName":"tingyu chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13919754884247043496"}},"outputId":"15157102-87ac-4805-e2fa-fb63ed1ca19d"},"execution_count":130,"outputs":[{"output_type":"stream","name":"stdout","text":["main.py:687: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n","  default_arg = yaml.load(f)\n","[ Mon Mar 21 23:55:40 2022 ] Model total number of params: 3194595\n","Cannot parse global_step from model weights filename\n","[ Mon Mar 21 23:55:40 2022 ] Loading weights from pretrained-models/ntu60-xsub-joint-fusion.pt\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/MS-G3D/feeders/feeder.py\", line 44, in load_data\n","    with open(self.label_path) as f:\n","FileNotFoundError: [Errno 2] No such file or directory: './data/ntu/xsub/val_label.pkl'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"main.py\", line 702, in <module>\n","    main()\n","  File \"main.py\", line 697, in main\n","    processor = Processor(arg)\n","  File \"main.py\", line 253, in __init__\n","    self.load_data()\n","  File \"main.py\", line 398, in load_data\n","    dataset=Feeder(**self.arg.test_feeder_args),\n","  File \"/content/drive/MyDrive/MS-G3D/feeders/feeder.py\", line 37, in __init__\n","    self.load_data()\n","  File \"/content/drive/MyDrive/MS-G3D/feeders/feeder.py\", line 48, in load_data\n","    with open(self.label_path, 'rb') as f:\n","FileNotFoundError: [Errno 2] No such file or directory: './data/ntu/xsub/val_label.pkl'\n","main.py:687: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n","  default_arg = yaml.load(f)\n","[ Mon Mar 21 23:55:47 2022 ] Model total number of params: 3194595\n","Cannot parse global_step from model weights filename\n","[ Mon Mar 21 23:55:47 2022 ] Loading weights from pretrained-models/ntu60-xsub-bone.pt\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/MS-G3D/feeders/feeder.py\", line 44, in load_data\n","    with open(self.label_path) as f:\n","FileNotFoundError: [Errno 2] No such file or directory: './data/ntu/xsub/val_label.pkl'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"main.py\", line 702, in <module>\n","    main()\n","  File \"main.py\", line 697, in main\n","    processor = Processor(arg)\n","  File \"main.py\", line 253, in __init__\n","    self.load_data()\n","  File \"main.py\", line 398, in load_data\n","    dataset=Feeder(**self.arg.test_feeder_args),\n","  File \"/content/drive/MyDrive/MS-G3D/feeders/feeder.py\", line 37, in __init__\n","    self.load_data()\n","  File \"/content/drive/MyDrive/MS-G3D/feeders/feeder.py\", line 48, in load_data\n","    with open(self.label_path, 'rb') as f:\n","FileNotFoundError: [Errno 2] No such file or directory: './data/ntu/xsub/val_label.pkl'\n","main.py:687: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n","  default_arg = yaml.load(f)\n","[ Mon Mar 21 23:55:54 2022 ] Model total number of params: 3194595\n","Cannot parse global_step from model weights filename\n","[ Mon Mar 21 23:55:54 2022 ] Loading weights from pretrained-models/ntu60-xview-joint.pt\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/MS-G3D/feeders/feeder.py\", line 44, in load_data\n","    with open(self.label_path) as f:\n","FileNotFoundError: [Errno 2] No such file or directory: './data/ntu/xview/val_label.pkl'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"main.py\", line 702, in <module>\n","    main()\n","  File \"main.py\", line 697, in main\n","    processor = Processor(arg)\n","  File \"main.py\", line 253, in __init__\n","    self.load_data()\n","  File \"main.py\", line 398, in load_data\n","    dataset=Feeder(**self.arg.test_feeder_args),\n","  File \"/content/drive/MyDrive/MS-G3D/feeders/feeder.py\", line 37, in __init__\n","    self.load_data()\n","  File \"/content/drive/MyDrive/MS-G3D/feeders/feeder.py\", line 48, in load_data\n","    with open(self.label_path, 'rb') as f:\n","FileNotFoundError: [Errno 2] No such file or directory: './data/ntu/xview/val_label.pkl'\n","main.py:687: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n","  default_arg = yaml.load(f)\n","[ Mon Mar 21 23:56:01 2022 ] Model total number of params: 3194595\n","Cannot parse global_step from model weights filename\n","[ Mon Mar 21 23:56:01 2022 ] Loading weights from pretrained-models/ntu60-xview-bone.pt\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/MS-G3D/feeders/feeder.py\", line 44, in load_data\n","    with open(self.label_path) as f:\n","FileNotFoundError: [Errno 2] No such file or directory: './data/ntu/xview/val_label.pkl'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"main.py\", line 702, in <module>\n","    main()\n","  File \"main.py\", line 697, in main\n","    processor = Processor(arg)\n","  File \"main.py\", line 253, in __init__\n","    self.load_data()\n","  File \"main.py\", line 398, in load_data\n","    dataset=Feeder(**self.arg.test_feeder_args),\n","  File \"/content/drive/MyDrive/MS-G3D/feeders/feeder.py\", line 37, in __init__\n","    self.load_data()\n","  File \"/content/drive/MyDrive/MS-G3D/feeders/feeder.py\", line 48, in load_data\n","    with open(self.label_path, 'rb') as f:\n","FileNotFoundError: [Errno 2] No such file or directory: './data/ntu/xview/val_label.pkl'\n","main.py:687: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n","  default_arg = yaml.load(f)\n","[ Mon Mar 21 23:56:08 2022 ] Model total number of params: 3217695\n","Cannot parse global_step from model weights filename\n","[ Mon Mar 21 23:56:08 2022 ] Loading weights from pretrained-models/ntu120-xsub-joint.pt\n","[ Mon Mar 21 23:56:09 2022 ] Model:   model.msg3d.Model\n","[ Mon Mar 21 23:56:09 2022 ] Weights: pretrained-models/ntu120-xsub-joint.pt\n","[ Mon Mar 21 23:56:09 2022 ] Eval epoch: 1\n","100% 1/1 [00:00<00:00,  2.30it/s]\n","Accuracy:  0.0  model:  pretrain_eval/ntu120/xsub/joint\n","[ Mon Mar 21 23:56:10 2022 ] \tMean test loss of 1 batches: 1454.1156005859375.\n","[ Mon Mar 21 23:56:10 2022 ] \tTop 1: 0.00%\n","[ Mon Mar 21 23:56:10 2022 ] \tTop 5: 0.00%\n","[ Mon Mar 21 23:56:10 2022 ] Done.\n","\n","main.py:687: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n","  default_arg = yaml.load(f)\n","[ Mon Mar 21 23:56:15 2022 ] Model total number of params: 3217695\n","Cannot parse global_step from model weights filename\n","[ Mon Mar 21 23:56:15 2022 ] Loading weights from pretrained-models/ntu120-xsub-bone.pt\n","[ Mon Mar 21 23:56:17 2022 ] Model:   model.msg3d.Model\n","[ Mon Mar 21 23:56:17 2022 ] Weights: pretrained-models/ntu120-xsub-bone.pt\n","[ Mon Mar 21 23:56:17 2022 ] Eval epoch: 1\n","100% 1/1 [00:00<00:00,  2.71it/s]\n","Accuracy:  0.0  model:  pretrain_eval/ntu120/xsub/bone\n","[ Mon Mar 21 23:56:17 2022 ] \tMean test loss of 1 batches: 735.7554931640625.\n","[ Mon Mar 21 23:56:17 2022 ] \tTop 1: 0.00%\n","[ Mon Mar 21 23:56:17 2022 ] \tTop 5: 0.00%\n","[ Mon Mar 21 23:56:17 2022 ] Done.\n","\n","main.py:687: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n","  default_arg = yaml.load(f)\n","[ Mon Mar 21 23:56:23 2022 ] Model total number of params: 3217695\n","Cannot parse global_step from model weights filename\n","[ Mon Mar 21 23:56:23 2022 ] Loading weights from pretrained-models/ntu120-xset-joint.pt\n","[ Mon Mar 21 23:56:24 2022 ] Model:   model.msg3d.Model\n","[ Mon Mar 21 23:56:24 2022 ] Weights: pretrained-models/ntu120-xset-joint.pt\n","[ Mon Mar 21 23:56:24 2022 ] Eval epoch: 1\n","100% 1/1 [00:00<00:00,  2.90it/s]\n","Accuracy:  0.0  model:  pretrain_eval/ntu120/xset/joint\n","[ Mon Mar 21 23:56:25 2022 ] \tMean test loss of 1 batches: 1377.0411376953125.\n","[ Mon Mar 21 23:56:25 2022 ] \tTop 1: 0.00%\n","[ Mon Mar 21 23:56:25 2022 ] \tTop 5: 0.00%\n","[ Mon Mar 21 23:56:25 2022 ] Done.\n","\n","main.py:687: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n","  default_arg = yaml.load(f)\n","[ Mon Mar 21 23:56:30 2022 ] Model total number of params: 3217695\n","Cannot parse global_step from model weights filename\n","[ Mon Mar 21 23:56:30 2022 ] Loading weights from pretrained-models/ntu120-xset-bone.pt\n","[ Mon Mar 21 23:56:32 2022 ] Model:   model.msg3d.Model\n","[ Mon Mar 21 23:56:32 2022 ] Weights: pretrained-models/ntu120-xset-bone.pt\n","[ Mon Mar 21 23:56:32 2022 ] Eval epoch: 1\n","100% 1/1 [00:00<00:00,  2.97it/s]\n","Accuracy:  0.0  model:  pretrain_eval/ntu120/xset/bone\n","[ Mon Mar 21 23:56:32 2022 ] \tMean test loss of 1 batches: 966.7288818359375.\n","[ Mon Mar 21 23:56:32 2022 ] \tTop 1: 0.00%\n","[ Mon Mar 21 23:56:32 2022 ] \tTop 5: 0.00%\n","[ Mon Mar 21 23:56:32 2022 ] Done.\n","\n","main.py:687: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n","  default_arg = yaml.load(f)\n","[ Mon Mar 21 23:56:38 2022 ] Model total number of params: 3144328\n","Cannot parse global_step from model weights filename\n","[ Mon Mar 21 23:56:38 2022 ] Loading weights from pretrained-models/kinetics-joint.pt\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/MS-G3D/feeders/feeder.py\", line 44, in load_data\n","    with open(self.label_path) as f:\n","FileNotFoundError: [Errno 2] No such file or directory: './data/kinetics/val_label.pkl'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"main.py\", line 702, in <module>\n","    main()\n","  File \"main.py\", line 697, in main\n","    processor = Processor(arg)\n","  File \"main.py\", line 253, in __init__\n","    self.load_data()\n","  File \"main.py\", line 398, in load_data\n","    dataset=Feeder(**self.arg.test_feeder_args),\n","  File \"/content/drive/MyDrive/MS-G3D/feeders/feeder.py\", line 37, in __init__\n","    self.load_data()\n","  File \"/content/drive/MyDrive/MS-G3D/feeders/feeder.py\", line 48, in load_data\n","    with open(self.label_path, 'rb') as f:\n","FileNotFoundError: [Errno 2] No such file or directory: './data/kinetics/val_label.pkl'\n","main.py:687: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n","  default_arg = yaml.load(f)\n","[ Mon Mar 21 23:56:45 2022 ] Model total number of params: 3144328\n","Cannot parse global_step from model weights filename\n","[ Mon Mar 21 23:56:45 2022 ] Loading weights from pretrained-models/kinetics-bone.pt\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/MS-G3D/feeders/feeder.py\", line 44, in load_data\n","    with open(self.label_path) as f:\n","FileNotFoundError: [Errno 2] No such file or directory: './data/kinetics/val_label.pkl'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"main.py\", line 702, in <module>\n","    main()\n","  File \"main.py\", line 697, in main\n","    processor = Processor(arg)\n","  File \"main.py\", line 253, in __init__\n","    self.load_data()\n","  File \"main.py\", line 398, in load_data\n","    dataset=Feeder(**self.arg.test_feeder_args),\n","  File \"/content/drive/MyDrive/MS-G3D/feeders/feeder.py\", line 37, in __init__\n","    self.load_data()\n","  File \"/content/drive/MyDrive/MS-G3D/feeders/feeder.py\", line 48, in load_data\n","    with open(self.label_path, 'rb') as f:\n","FileNotFoundError: [Errno 2] No such file or directory: './data/kinetics/val_label.pkl'\n","\n","NTU RGB+D 60 XSub\n","Traceback (most recent call last):\n","  File \"ensemble.py\", line 28, in <module>\n","    with open('/content/drive/My Drive/MS-G3D/data/' + dataset + '/val_label.pkl', 'rb') as label:\n","FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/My Drive/MS-G3D/data/ntu/xsub/val_label.pkl'\n","\n","NTU RGB+D 60 XView\n","Traceback (most recent call last):\n","  File \"ensemble.py\", line 28, in <module>\n","    with open('/content/drive/My Drive/MS-G3D/data/' + dataset + '/val_label.pkl', 'rb') as label:\n","FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/My Drive/MS-G3D/data/ntu/xview/val_label.pkl'\n","\n","NTU RGB+D 120 XSub\n","100% 2/2 [00:00<00:00, 8240.28it/s]\n","Top1 Acc: 0.0000%\n","Top5 Acc: 0.0000%\n","\n","NTU RGB+D 120 XSet\n","100% 2/2 [00:00<00:00, 6369.48it/s]\n","Top1 Acc: 0.0000%\n","Top5 Acc: 0.0000%\n","\n","Kinetics Skeleton 400\n","Traceback (most recent call last):\n","  File \"ensemble.py\", line 28, in <module>\n","    with open('/content/drive/My Drive/MS-G3D/data/' + dataset + '/val_label.pkl', 'rb') as label:\n","FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/My Drive/MS-G3D/data/kinetics/val_label.pkl'\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Dectron+VideoPose.ipynb","provenance":[],"authorship_tag":"ABX9TyO+Pjee0jAaz5k10qugXeOL"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}